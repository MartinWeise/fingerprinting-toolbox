{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Analysis on Machine Learning Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents the utility analysis of the proposed fingerprinting method as a difference in the performance of an ML algorithm trained on original dataset and its fingerprinted copy. \n",
    "Since the fingerprinting methods necessarily change the values inside of the dataset, it is our aim as the designers of the method to keep the quality of the data as close to the original as possible. \n",
    "\n",
    "In our experiments we will train the ML models on the original data and test the performance of the final model on the holdout test set - we want to mock the real scenario when the recipient of the data will have only the fingerprinted data to train their model and to use that model to predict the new, never seen set. Then we will train the model with same hyperparameters using the fingerprinted dataset. \n",
    "\n",
    "We will test the utility of our fingeprinted models using different types  of classifiers, namely:\n",
    "* Decision Tree\n",
    "* Logistic Regression\n",
    "* Gradient Boosting\n",
    "\n",
    "Our experiments will have the following workflow:\n",
    "* choose a random holdout test set (20%)\n",
    "* train the classifier on the remaining & cross validate \n",
    "* record the performance on the test set\n",
    "* fingerprint the remaining \n",
    "* train the classifier on the fingerprinted & cross validate\n",
    "* record the performance on the (non-fingerprinted) test set\n",
    "\n",
    "This is to be repeated a number of times (eg. 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need this for correct imports\n",
    "import sys, os\n",
    "if 'C:/Users/tsarcevic/PycharmProjects/fingerprinting-toolbox/' not in sys.path:\n",
    "    sys.path.append('C:/Users/tsarcevic/PycharmProjects/fingerprinting-toolbox/')\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree on German Credit data (1000 entries)\n",
    "\n",
    "We first present one run of our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: datasets/german_credit_full.csv\n"
     ]
    }
   ],
   "source": [
    "# i divide a random portion of data (20%) and keep it until the test phase\n",
    "dataset_name = \"german_credit_full\"\n",
    "german_credit, primary_key = import_dataset(dataset_name)\n",
    "# take a random holdout\n",
    "holdout = german_credit.sample(frac=0.2, random_state=0)\n",
    "german_credit = german_credit.drop(holdout.index, axis=0)\n",
    "\n",
    "# one-hot encode the categorical vals but first label-encode the catergorical because OneHot cant't handle them LOL\n",
    "label_enc = LabelEncoder()\n",
    "categorical_attributes = german_credit.select_dtypes(include='object').columns\n",
    "for cat in categorical_attributes:\n",
    "    german_credit[cat] = label_enc.fit_transform(german_credit[cat])\n",
    "    holdout[cat] = label_enc.fit_transform(holdout[cat])\n",
    "c = len(german_credit.columns)\n",
    "data = german_credit.values[:, 1:(c-1)]\n",
    "target = german_credit.values[:, (c-1)]\n",
    "\n",
    "categorical_features_idx = [i-1 for i in range(len(german_credit.columns)) if german_credit.columns[i] in categorical_attributes]\n",
    "encoder = OneHotEncoder(categorical_features=categorical_features_idx)\n",
    "data = encoder.fit_transform(data).toarray().astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our model and define possible hyperparameters for the grid seach. With the grid search we will determine which hyperparameters show to obtain the model with the best performance on the given data. We test the performance (accurace, f1) via cross validation.\n",
    "We do not stress our analysis on finding the best possible model; our goal is simply to see the difference in the performance when the exact same model is trained with fingerprinted data - \"good-enough\" model will do for this purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "criterion_range = [\"gini\", \"entropy\"]\n",
    "max_depth_range = range(1, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will serch for the best hyperparameters based on F1 score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best F1 score achieved is: 0.8317228925455917 with hyperparameters: {'max_depth': 3, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "param_dist = dict(criterion=criterion_range, max_depth=max_depth_range)\n",
    "rand = RandomizedSearchCV(model, param_dist, cv=10, n_iter=20, scoring=\"f1\", random_state=0)\n",
    "rand.fit(data, target)\n",
    "print(\"The best F1 score achieved is: \" + str(rand.best_score_) + \" with hyperparameters: \" + str(rand.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy achieved is: 0.72375 with hyperparameters: {'max_depth': 3, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "rand = RandomizedSearchCV(model, param_dist, cv=10, n_iter=20, scoring=\"accuracy\", random_state=0)\n",
    "rand.fit(data, target)\n",
    "print(\"The best accuracy achieved is: \" + str(rand.best_score_) + \" with hyperparameters: \" + str(rand.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both performance measures achieved the peak on the setting (max_depth=3, criterion=entropy) so we will use these hyperparameters for our Decision Tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model = DecisionTreeClassifier(random_state=0, criterion='entropy', max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=3,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = holdout.values[:, 1:(c-1)]\n",
    "X_test = encoder.fit_transform(X_test).toarray().astype(np.int)\n",
    "y_test = holdout.values[:, (c-1)]\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our accuracy for the model trained with the original data.\n",
    "Now we use the same hyperparameters to build the Decision Tree model and train it on fingerprinted data. Let us choose the dataset that is fingerprinted with gamma=7 and xi=2. We will use the same subset for training as in the first case. \n",
    "\n",
    "#### Fingerprinted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_credit_fp = read_data_with_target(\"german_credit\", \"categorical_neighbourhood\", [7, 2], 0)\n",
    "# remove the holdout\n",
    "german_credit_fp = german_credit_fp.drop(holdout.index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "# one-hot encode the categorical vals but first label-encode the catergorical because OneHot cant't handle them LOL\n",
    "label_enc = LabelEncoder()\n",
    "for cat in categorical_attributes:\n",
    "    german_credit_fp[cat] = label_enc.fit_transform(german_credit_fp[cat])\n",
    "data_fp = german_credit_fp.values[:, 1:(c-1)]\n",
    "target_fp = german_credit_fp.values[:, (c-1)]\n",
    "\n",
    "encoder = OneHotEncoder(categorical_features=categorical_features_idx)\n",
    "data_fp = encoder.fit_transform(data_fp).toarray().astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our model with the same hyperparameters as above and train it on fingerprinted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(random_state=0, criterion='entropy', max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=3,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data_fp, target_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we trained the model, let us see the performance on the holdout set (which is not fingerprinted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.715"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy obtained is 0.715 -> smaller than our original model. \n",
    "\n",
    "This experiment should be performed a number of times to be able to generalize the conclusions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression on German Credit data (1000 entries)\n",
    "\n",
    "Now we want to do essentially the same thing as described previously, but this time testing on Logistic Regression performance. The workflow stays the same. We will reuse the above defined object for a faster execution. \n",
    "\n",
    "#### Original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "model = LogisticRegression(random_state=0, max_iter=200)\n",
    "\n",
    "solver_range = [\"liblinear\", \"newton-cg\", \"lbfgs\", \"saga\"]\n",
    "C_range = range(10, 101, 10)\n",
    "param_dist = dict(C=C_range, solver=solver_range)\n",
    "rand = RandomizedSearchCV(model, param_dist, cv=10, n_iter=10, scoring=\"f1\", random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best F1 score achieved is: 0.8282794656372932 with hyperparameters: {'solver': 'liblinear', 'C': 60}\n"
     ]
    }
   ],
   "source": [
    "rand.fit(data, target)\n",
    "print(\"The best F1 score achieved is: \" + str(rand.best_score_) + \" with hyperparameters: \" + str(rand.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy achieved is: 0.75 with hyperparameters: {'solver': 'liblinear', 'C': 60}\n"
     ]
    }
   ],
   "source": [
    "rand = RandomizedSearchCV(model, param_dist, cv=10, n_iter=10, scoring=\"accuracy\", random_state=0)\n",
    "rand.fit(data, target)\n",
    "print(\"The best accuracy achieved is: \" + str(rand.best_score_) + \" with hyperparameters: \" + str(rand.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the model with hyperparameters (solver=liblinear and C=60)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=60, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=200, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "model = LogisticRegression(random_state=0, max_iter=200, solver=\"liblinear\", C=60)\n",
    "model.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fingerprinted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=60, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=200, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=0, max_iter=200, solver=\"liblinear\", C=60)\n",
    "model.fit(data_fp, target_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.755"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model built on fingerprinted data seems to be better than the model buit on original data, which might be the result of a pure coincidence. For the real results, this experiment needs to be repeated multiple times."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
