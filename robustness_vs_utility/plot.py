import matplotlib.pyplot as plt
import numpy as np

full = 0.7746131474648029
n_exp = 250
x = np.array([i for i in range(9)])
y_robustness = n_exp - np.flip(np.array([250, 250, 250, 250, 243, 232, 226, 201, 0]))

y_logistic_regression = []
# ------------------------------------------------------------------ #
# DECISION TREE #
# ------------------------------------------------------------------ #
y_decision_tree = [0,
                   np.mean([0.5072753196019532, 0.5119196586267205, 0.5108535016477451, 0.5122545411966659,
                            0.5131778329765307, 0.5129381025272329, 0.5129934211056413, 0.5132096168077617,
                            0.5131332874150003, 0.5131833721732453, 0.513223942497469, 0.5132871646222487,
                            0.5132670146431608, 0.5132350840222245, 0.5131797663684317]),
                   np.mean([0.5115285976125241, 0.5118833135641151, 0.5108654976621795, 0.5122532389733659,
                            0.5131773037036441, 0.5129389118254578, 0.5129992082760457, 0.5132085232457501,
                            0.5131355121138589, 0.5131832063467446, 0.5132254368761183, 0.5132856012649989,
                            0.5132670952241479, 0.5132382969248516, 0.5131799996477037]),
                   np.mean([0.5221901316526837, 0.5119574542003423, 0.5108982221837649, 0.5122470355467172,
                            0.5131335708791956, 0.512926063175189, 0.5130112190756544, 0.5132145249333501,
                            0.5131429060838943, 0.5131734493523028, 0.513228633297775, 0.5132810047619102,
                            0.5132654068013964, 0.5132442048080638, 0.5131868026963877]),
                   np.mean([0.53485472741062, 0.5128924575172481, 0.5127964346532607, 0.5122468710765228,
                            0.513071746452865, 0.5129190936952633, 0.5129923407784839, 0.5131989513526015,
                            0.5131579662305376, 0.513144283610365, 0.5132181850716808, 0.5132744178682109,
                            0.5132613695872765, 0.5132461089649252, 0.5131942760523698]),
                   np.mean([0.5738258403322982, 0.5071876022703782, 0.5122440139614807, 0.5113903567549279,
                            0.5129936802917603, 0.5129588099949199, 0.5129965905321174, 0.5131503546976268,
                            0.5131762014454648, 0.5131141338850387, 0.5132090940271343, 0.5132504705282369,
                            0.5132716241468862, 0.5132504073301828, 0.5132097705294311]),
                   np.mean([0.6055373866481127, 0.5077391526729611, 0.5116468372281179, 0.5110198694219092,
                            0.51223317972143, 0.5130610377725118, 0.512976287772564, 0.5130706127944621,
                            0.5131780656155001, 0.5131106438962603, 0.5132016215370029, 0.5132344561911556,
                            0.5132779732821429, 0.5132627609428051, 0.5132196685682396]),
                   np.mean([0.6579239787816992, 0.5075159786069399, 0.5118053920867379, 0.5109094085777769,
                            0.5122175595776423, 0.51314003679712, 0.5129467604904417, 0.5130206757279465,
                            0.5131974854494507, 0.5131236295786407, 0.5132023619144298, 0.5132229265762479,
                            0.5132844282681346, 0.5132672662145233, 0.5132304548452067]),
                   0.7829475308641975]

y_max_decision_tree = [0,
                       np.mean([0.8548330130400961, 0.8608534741280909, 0.8574308704222702, 0.8603695233020532,
                                0.8595404266492588, 0.8599378952339797, 0.8594282673887307, 0.8599619727172272,
                                0.8591607709064026, 0.8589432399506449, 0.8585908745684553, 0.8590303419884996,
                                0.8587603715784097, 0.8584851355177896, 0.8584625916824383]),
                       np.mean([0.8548330130400961, 0.8608534741280909, 0.8574308704222702, 0.8603695233020532,
                                0.8595404266492588, 0.8599378952339797, 0.8594282673887307, 0.8599619727172272,
                                0.8591607709064026, 0.8589432399506449, 0.8585908745684553, 0.8590303419884996,
                                0.8587603715784097, 0.8584851355177896, 0.8584625916824383]),
                       np.mean([0.8548330130400961, 0.8608534741280909, 0.8574308704222702, 0.8603695233020532,
                               0.8595404266492588, 0.8599378952339797, 0.8594282673887307, 0.8599619727172272,
                               0.8591607709064026, 0.8589432399506449, 0.8585908745684553, 0.8590303419884996,
                               0.8587603715784097, 0.8584851355177896, 0.8584625916824383]),
                       np.mean([0.8548330130400961, 0.8608534741280909, 0.8574308704222702, 0.8603695233020532,
                                0.8595404266492588, 0.8599378952339797, 0.8594282673887307, 0.8599619727172272,
                                0.8591607709064026, 0.8589432399506449, 0.8585908745684553, 0.8590303419884996,
                                0.8587603715784097, 0.8584851355177896, 0.8584625916824383]),
                       np.mean([0.8548330130400961, 0.8608534741280909, 0.8574308704222702, 0.8603695233020532,
                                0.8595404266492588, 0.8599378952339797, 0.8594282673887307, 0.8599619727172272,
                                0.8591607709064026, 0.8589432399506449, 0.8585908745684553, 0.8590303419884996,
                                0.8587603715784097, 0.8584851355177896, 0.8584625916824383]),
                       np.mean([0.8548330130400961, 0.8608534741280909, 0.8574308704222702, 0.8603695233020532,
                                0.8595404266492588, 0.8599378952339797, 0.8594282673887307, 0.8599619727172272,
                                0.8591607709064026, 0.8589432399506449, 0.8585908745684553, 0.8590303419884996,
                                0.8587603715784097, 0.8584851355177896, 0.8584625916824383]),
                       np.mean([0.8548330130400961, 0.8608534741280909, 0.8574308704222702, 0.8603695233020532,
                                0.8595404266492588, 0.8599378952339797, 0.8594282673887307, 0.8599619727172272,
                                0.8591607709064026, 0.8589432399506449, 0.8585908745684553, 0.8590303419884996,
                                0.8587603715784097, 0.8584851355177896, 0.8584625916824383]),
                       0.7829475308641975]
print(y_decision_tree)
# ------------------------------------------------------------------ #
# k-NN #
# ------------------------------------------------------------------ #
y_knn = [0, 0.5934317175031674, 0.5680700227923975, 0.5377756563543514, 0.5156905832553836, 0.505677198406211,
         0.4941711923427788, 0.48963037480347266, 0.7689]
# ------------------------------------------------------------------ #
# GRADIENT BOOSTING #
# ------------------------------------------------------------------ #
y_gradient_boosting = []

fig, ax = plt.subplots(1, 1, sharex='col', sharey='row')
# the grey scale
# ax.plot(x, y_robustness/n_exp, label='$\gamma$ = 5', c='0.15')

ax.plot(x, y_robustness/n_exp, label='$\gamma$ = 5')
ax.plot(x, y_knn, label='KNN')


plt.xticks(np.arange(0, 9, step=1), [0, 1, 2, 3, 4, 5, 6, 7, 'ALL'])
fig.text(0.5, 0.02, 'Number of columns released', ha='center', size=14)
fig.text(0.04, 0.5, 'False Miss / Classification accuracy', va='center', rotation='vertical', size=14)
ax.legend()

plt.show()
